#! /usr/bin/env bash

: <<=cut
=pod

=head1  NAME

harvest - overall harvest command

=head1 SYNOPSIS

harvest [-b|--base=<base>]
  <command> [<args>]

  where <command> is one of db,load,update,query,login,run,export,config.


=head1 GLOBAL OPTIONS

=over 4

=item B<-b|--base = directory>

Specify the base directory for your feed results. Default C<base=./>.


=item B<-v|--verbose>

Log some of the commands that your are going to run.

=item B<-h|--help>

Shows the manpage for the program. The help pages are embedded in the script and
require the functions, C<pod2usage> and C<pod2text> to work properly.

=back

=cut

function main.init() {

  #MAIN
    # global Variables
  declare -g -A G=(
		[harvest_base]=/var/lib/harvest
		[cdl_auth]=ucd:unspecified
		[ucdid_auth]=ucd:unspecified
    [harvest_home]="$( cd "$( dirname "${BASH_SOURCE[0]}" )/../lib/harvest" >/dev/null 2>&1 && pwd )"
    [iam_api_endpoint]="iam.api:=https://iet-ws.ucdavis.edu/api/iam"
    [iam_search]=""
    [fuseki_host]='http://fuseki:3030'
    [harvest_db]=''
    [fuseki_admin]='http://fuseki:8080'
    [fuseki_user]='admin'
    [fuseki_password]=''
    [fuseki_auth]=''
    [util_getopt]=${HARVEST_UTIL_GETOPT:-${FLAGS_GETOPT_CMD:-getopt}}
    );

    local opts;
    if ! opts=$(${G[util_getopt]} -o f:b:vh --long db:,fuseki-host:,fuseki-auth:,base:,verbose,help -n 'harvest' -- "$@"); then
    echo "Bad Command Options." >&2 ; exit 1 ; fi

    eval set -- "$opts"

    local i
    local v
    declare -A CMD=(
			[verbose]=0
			);
    while true; do
	    case $1 in
	      -b | --base) CMD[harvest_base]=$2;  shift 2;;
	      -f | --fuseki-host) CMD[fuseki_host]=$2;  shift 2;;
	      -d | --db) CMD[harvest_db]=$2;  shift 2;;
	      --fuseki-admin) CMD[fuseki_admin]=$2;  shift 2;;
	      -a | --fuseki-auth) CMD[fuseki_auth]=$2;  shift 2;;
	      -v | --verbose) (( CMD[verbose]=CMD[verbose]+1 ));  shift;;
        -h | --help ) exec pod2text "$0";;
	      -- ) shift; break;;
	      *) shift; break;
      esac
    done

    # system variables (ELEMENTS_FOO_BAR) over config file
    # the HARVEST command uses root variables for these input, that is, eg
    # FUSEKI_PASSWORD, etc.
    for i in "${!G[@]}"; do
      eval v=\$${i^^}
      [[ -n $v ]] && G[$i]=$v
    done

    # command line over config and over system var
    for i in "${!CMD[@]}"; do
      [[ -n ${CMD[$i]} ]] && G[$i]=${CMD[$i]};
    done

    # Now build an auth if needed
    [[ -n ${G[fuseki_auth]} ]] || G[fuseki_auth]="${G[fuseki_user]}:${G[fuseki_password]}"

}

: <<='cut'

=pod

=head1 COMMANDS

Next there are a set of commands that communicate with the CDL service. Note,
that ezid uses L<httpie|https://httpie.org/> for its http communcation. This
allows users to combine ezid with other httpie requests if required. Login
infomation is stored using the standard C<httpie> methodology, see L</"GLOBAL OPTIONS"> for httpid options.

C<elements [options] harvest --full> Harvests a number of feeds from the
Elements database and stores each record in a separate file.  This is to match how
the Sympletic Harvester works

=cut

function main.cmd () {
    cmd=$1
    shift;
    case $cmd in
	    xslt) # VIVO Harvester
	      $cmd "$@";
	      ;;
      db | load | update | query ) # Fuseki commands
        $cmd "$@";
        ;;
			login) # API commands
				$cmd "$@"
				;;
			run ) # Import a set of people
        $cmd "$@";
        ;;
			export ) # Export Experts database.
        $cmd "$@";
        ;;
      config ) # informational requests
        declare -p G;
        ;;
	    *)
	      exec pod2usage $0
	  ;;
    esac
}

function log() {
  local opts;
  if ! opts=$(${G[util_getopt]} -o nv --long verbose -n 'harvest log' -- "$@"); then
    echo "Bad Command Options." >&2 ; exit 1 ;
	fi

  eval set -- "$opts"

  local log_verbose=0
  local parm_n=''

  while true; do
	  case $1 in
	    -v | --verbose) let log_verbose=$log_verbose+1;  shift;;
	    -n ) parm_n='-n';  shift;;
	    -- ) shift; break;;
	    *) shift; break;
    esac
  done
	(( G[verbose] >= "$log_verbose" )) && (echo $parm_n "$@")
}

function err() {
  local n=1;
  if [[ $1 = '--quiet' ]] ; then
    n=$2;
  else
    n=$1
    shift
    (>&2 echo err: "$@")
  fi
  exit "$n";
}

: <<='cut'
=pod

=head2 COMMAND

harvest I<options> B<db> list|rm|new [service]

Performs processes on the fuseki instance.  These are command-line overlays to
the harvetstdb admin API in the ucd-rp-fuseki host.


=head3 COMMANDS

=over 4

=item B<list>

List all fuseki harvest services.  The list comes from the existing harvest
databases, not the services running in the server.

=item B<new> [service]

Add a new harvest service.  This will add a standard database, with a private
section, as well as access to the public data.

=item B<rm> [service]

Delete a harvest service.  This will remove the configuration, and the
underlying database file.  This takes advantage of the L<fuseki HTTP adminsration protocol|https://jena.apache.org/documentation/fuseki2/fuseki-server-protocol.html>

=back

=cut

function db() {
  local cmd

  local auth=${G[fuseki_auth]}
  local dbs=()

  cmd=$1
  shift;

  case $cmd in
    datasets)
      http --print=b --auth=${auth} GET ${G[fuseki_host]}/\$/datasets | jq '.datasets[]["ds.name"]'
      ;;
    list)
      dbs=($(http --print=b --auth=${auth} GET ${G[fuseki_admin]}/harvestdb | tr -d "\r" | grep ...))
      ;;
    rm)
      while [[ -n "$1" ]]; do
        dbs+=($(http --auth=${auth} DELETE ${G[fuseki_admin]}/harvestdb?name="$1" | tr -d '\r'))
        if [[ -d ${G[harvest_base]}/"$1" ]]; then
          log -v "rm -rf ${G[harvest_base]}/$1"
          rm -rf "${G[harvest_base]}/$1"
        fi
        shift
      done
      ;;
    new)
      if [[ -z "$1" ]]; then
        dbs+=($(http --auth=${auth} POST ${G[fuseki_admin]}/harvestdb | tr -d '\r'))
      else
        while [[ -n "$1" ]]; do
          dbs+=($(http --auth=${auth} POST ${G[fuseki_admin]} /harvestdb?name="$1" | tr -d '\r'))
          shift
        done
      fi
      ;;
    *) err "Invalid command $cmd"
       ;;
  esac
  echo "${dbs[@]}"
}


: <<='cut'
=pod

=head2 COMMAND

harvest I<-v> I<-v> login I<--cdl_auth=cdl_auth> -I<--ucdid_auth=ucdid_auth>

Logs into the various required APIs.  You can specify the authentications on the
command line.  If these parameters are not specified, then uses the
environment variables C<CDL_AUTH> and C<UCDID_AUTH> respectively.  This is just
a convenience function that calls C<cdl login> and C<ucdid login>.

=cut

function login() {
  local opts;
  if ! opts=$(${G[util_getopt]} -o c:u: --long cdl:,ucdid: -n 'harvest login' -- "$@"); then
    echo "Bad Command Options." >&2 ; exit 1 ; fi

  eval set -- "$opts"

  declare -A CMD=(
    [cdl_auth]=${G[cdl_auth]}
    [ucdid_auth]=${G[ucdid_auth]}
    );

  while true; do
	  case $1 in
	    -c | --cdl_auth) CMD[cdl_auth]=$2;  shift 2;;
	    -u | --ucdid_auth) CMD[ucdid_auth]=$2;  shift 2;;
	    -- ) shift; break;;
	    *) shift; break;
    esac
  done

	local v_s
  v_s=$(for i in $(seq ${G[verbose]} ); do echo -n " -v"; done)

	log -v -n "cdl."
	cdl ${v_s} login --auth=${CMD[cdl_auth]}
	log -v -n "ucdid."
	ucdid ${v_s} login --auth=${CMD[ucdid_auth]}
}


: <<='cut'
=pod

=head2 COMMAND

harvest I<global_options> B<run> I<options>

Run a complete harvest operation for a particular ucdid search.  This command
will first initialize a HARVEST_DB if one does not exist.  Then the program will
query the C<ucdid> with the the passed C<--search> query, and collect a set of
users.  It will then contact the CDL elements database (via the cdl command) and
download all user and publication information for these users.  Then the command
will run a series of sparql updates (via C<harvest update>) to update the
experts database.  The command will then clean up, by removing the HARVEST_DB,
and the cache of files downloaded from the services.

=over 4

=item B<--init>

Specify whether to initialize a new database.  By default, this will check to
see if there is a global database already set.  If not, then this will
initialize a new database.  If a global database exists, then use that.  This
can be set either with the B<HARVEST_DB> environment variable, or with the
C<--db=I<harvest_db>> global option.  If C<--init> is set, then create a new
harvest_db regardless of the existance of a current HARVEST_DB.

=item B<--search=<ucdid --search parameter>

Specify the initial ucdid query to get the list of users to harvest, and update.
This is passed as the C<--search> to the C<ucdid> command.  All users from this
command will be included in the harvest operations.

=item B<--last-modified=<only included items changed since this date-time>

The C<--last-modified> takes a ISO datetime string, and only includes items that
have been modified since that date.  If unspecified, all items from the query
will be updated.

=item B<--remove|--no-remove>

Specify whether to remove the HARVEST_DB service and the resultant cache files
after processing.

=back

=cut

function run () {
  local opts;
  if ! opts=$(${G[util_getopt]} -o is:rnm: --long init,no-init,search:,remove,no-remove,last-modified -n 'harvest load' -- "$@"); then
    echo "Bad Command Options." >&2 ; exit 1 ; fi

  eval set -- "$opts"

  declare -A CMD=(
    [init]=1
    [search]=''
    [remove]=1
    [modified]=''
    );

  while true; do
	  case $1 in
	    -i | --init) CMD[init]=1;  shift;;
	    --no-init) CMD[init]='';  shift;;
	    -s | --search) CMD[search]=$2;  shift 2;;
      -r | --remove) CMD[remove]=1; shift;;
      -n | --no-remove) CMD[remove]=''; shift;;
      -m | --last-modified) CMD[modified]=$2; shift 2;;
	    -- ) shift; break;;
	    *) Shift; break;
    esac
  done

  if [[ -z ${CMD[search]} ]]; then
    err 1 "harvest run, must specify a --search parameter.  See ucdid for details."
  fi

  if [[ -n ${CMD[init]} || -z ${G[db]} ]]; then
    G[harvest_db]=$(db new);
  fi
	local ep=${G[fuseki_host]}/${G[harvest_db]}

  if ${CMD[fetch]} ; then
    # Now fetch all the data w/ CDL
    local cache=${G[harvest_base]}/${G[harvest_db]}
	  log -v -v "cache: $cache"
    mkdir $cache;
  fi
    # call UCDID w/ query
    local profiles_ttl=$cache/profiles.ttl
    log -v -v "ucdid fetch --format=ttl --search=\"${CMD[search]}\" profiles > $profiles_ttl"
    ucdid fetch --format=ttl --search="${CMD[search]}" profiles > $profiles_ttl
    load --graph=http://iam.ucdavis.edu/ $profiles_ttl

  # Now run an update to convert IAM to vivo users
  update iam_experts

  # Now get all the casIDs, This doesn't use experts, but the harvestdb
  ids="$(aeq --endpoint="$ep"/sparql query --format=json <<<"select ?id where { {graph harvest_iam: {?u iam:userID ?id; filter(NOT EXISTS {?u iam:directory ?d }) } } UNION { graph harvest_iam: {[] iam:userID ?id; iam:directory/iam:displayName/iam:nameWwwFlag 'Y'. }}} order by ?id" | jq -r .results.bindings[].id.value | tr [:space:] ' ')"

	local v_s
  v_s="$(for i in $(seq ${G[verbose]} ); do echo -n " -v"; done)"
#  cdl $v_s --data=$ep login --auth=${CDL_AUTH}
  cdl $v_s --cache=$cache --data=$ep users --pubs $ids

  # Then, run the update scripts for new data
  update update_harvest_records
  update experts
  update journal publications pub_FoR pub_free keywords expert_concepts;

	if [[ -n ${CMD[remove]} ]]; then
		log -v "db rm ${G[harvest_db]};"
		db rm ${G[harvest_db]}
	fi;
}

: <<='cut'
=pod

=head2 COMMAND

harvest I<global_options> B<update> I<options>

Run a complete harvest operation for a particular ucdid search.  This command
will first initialize a HARVEST_DB if one does not exist.  Then the program will
query the C<ucdid> with the the passed C<--search> query, and collect a set of
users.  It will then contact the CDL elements database (via the cdl command) and
download all user and publication information for these users.  Then the command
will run a series of sparql updates (via C<harvest update>) to update the
experts database.  The command will then clean up, by removing the HARVEST_DB,
and the cache of files downloaded from the services.

=over 4

=item B<--init>

Specify whether to initialize a new database.  By default, this will check to
see if there is a global database already set.  If not, then this will
initialize a new database.  If a global database exists, then use that.  This
can be set either with the B<HARVEST_DB> environment variable, or with the
C<--db=I<harvest_db>> global option.  If C<--init> is set, then create a new
harvest_db regardless of the existance of a current HARVEST_DB.

This is B<not> the experts database, this is the harvest database.

=item B<--search=<ucdid --search parameter>

Specify the initial ucdid query to get the list of users to harvest, and update.
This is passed as the C<--search> to the C<ucdid> command.  All users from this
command will be included in the harvest operations.

=item B<--last-modified=<only included items changed since this date-time>

The C<--last-modified> takes a ISO datetime string, and only includes items that
have been modified since that date.  If unspecified, all items from the query
will be updated.

=item B<--remove|--no-remove>

Specify whether to remove the HARVEST_DB service and the resultant cache files
after processing.

=back

=cut

function update_user () {
  local opts;
  if ! opts=$(${G[util_getopt]} -o is:rnm: --long init,no-init,search:,remove,no-remove,last-modified -n 'harvest load' -- "$@"); then
    echo "Bad Command Options." >&2 ; exit 1 ; fi

  eval set -- "$opts"

  declare -A CMD=(
    [init]=0
    [remove]=1
    [modified]=''
    );

  while true; do
	  case $1 in
	    -i | --init) CMD[init]=1;  shift;;
	    --no-init) CMD[init]='';  shift;;
      -r | --remove) CMD[remove]=1; shift;;
      -n | --no-remove) CMD[remove]=''; shift;;
      -m | --last-modified) CMD[modified]=$2; shift 2;;
	    -- ) shift; break;;
	    *) break;
    esac
  done

  # If no passed data on the command line, then update all ids in the current harvest
  if [[ -z $1 ]]; then
    set -- $(aeq --endpoint=$ep/sparql query --format=json <<<"select ?id where { graph harvest_iam: {[] iam:userID ?id. }} order by ?id" | jq -r .results.bindings[].id.value | tr [:space:] ' ')
  fi

  if [[ -n ${CMD[init]} || -z ${G[db]} ]]; then
    G[harvest_db]=$(db new);
  fi
	local ep=http://fuseki:3030/${G[harvest_db]}

  # Now fetch all the data w/ CDL
  local cache=${G[harvest_base]}/${G[harvest_db]}
	log -v -v "cache: $cache"
  mkdir $cache;

  # call UCDID w/ query
  local profiles_ttl=$cache/profiles.ttl
  log -v -v "ucdid fetch --format=ttl --search=\"${CMD[search]}\" profiles > $profiles_ttl"
  ucdid fetch --format=ttl --search="${CMD[search]}" profiles > $profiles_ttl
  load --graph=http://iam.ucdavis.edu/ $profiles_ttl

  # Now run an update to convert IAM to vivo users
  update iam_to_vivo

  # Now get all the casIDs, This doesn't use experts, but the harvestdb

	local v_s
  v_s=$(for i in $(seq ${G[verbose]} ); do echo -n " -v"; done)
#  cdl $v_s --data=$ep login --auth=${CDL_AUTH}
  cdl $v_s --cache=$cache --data=$ep users --pubs $ids

  # Then, run the update scripts for new data
  update update_harvest_records
  update journal publications pub_FoR pub_free keywords expert_concepts;

	if [[ -n ${CMD[remove]} ]]; then
		log -v "db rm ${G[harvest_db]}; rm -rf $cache"
		db rm ${G[harvest_db]}
		rm -rf $cache
	fi;
}

: <<='cut'
=pod

=head2 COMMAND

harvest I<global_options> B<export> I<options>

Exports the experts database on a per graph basis.  This allows the data to be
shared with another implementation, or saved as an archive.

=over 4

=item B<--directory=export_directory>

If specified, then save to this directory. Otherwise uses
${HARVEST_BASE:-/var/lib/harvest}/export as the location.

=back

=cut

function export () {
  local opts;
  if ! opts=$(${G[util_getopt]} -o d: --long directory -n 'harvest export' -- "$@"); then
    echo "Bad Command Options." >&2 ; exit 1 ; fi

  eval set -- "$opts"

  declare -A CMD=(
    [directory]=''
    );

  while true; do
	  case $1 in
	    -d | --directory) CMD[directory]=$2;  shift 2;;
	    -- ) shift; break;;
	    *) shift; break;
    esac
  done

	if [[ -z ${CMD[directoy]} ]]; then
		 CMD[directory]=${G[harvest_base]}/export;
		 [[ -d ${CMD[directory]} ]] || mkdir ${CMD[directory]};
	fi

	local graphdirs=(
		experts.ucdavis.edu
		experts.ucdavis.edu%2Fiam
		experts.ucdavis.edu%2Foap
	)

	local get=http://fuseki:3030/experts/get

	for dir in ${graphdirs[@]}; do
		local d=${CMD[directory]}/$dir;
		[[ -d $d ]] || mkdir $d;
		local g=$(printf 'http://%b/' ${dir//%/\\x})
		log -v "curl \"${get}?graph=$g\" | gzip > $d/graph.ttl"
    curl -s "${get}?graph=$g" | gzip > $d/graph.ttl.gz
	done

}

: <<='cut'
=pod

=head2 COMMAND

harvest I<global_options> B<query> I<options> file1.ttl file2.ttl ...

Run queries on the fuseki harvestdb endpoint.  I

=cut

#function query () {
#}


: <<='cut'
=pod

=head2 COMMAND

harvest I<global_options> B<update> I<options> file1.ttl file2.ttl ...

Run update queries on the fuseki harvestdb endpoint.  I

=cut

function update () {
  local file;
	local begin;
  if [[ -n ${G[harvest_db]} ]]; then
    local auth=${G[fuseki_auth]}
    local update=${G[fuseki_host]}/${G[harvest_db]}/update

    # Short hand for files in lib/harvest
    for file in "$@"; do
      [[ "$file" = '-' ]] && file=/dev/stdin
      if [[ ! -f $file ]]; then
        file=${G[harvest_home]}/ru/$file;
        [[ -f $file ]] || file=${file}.ru
      fi
      if [[ -f $file ]]; then
				log -v -v "curl -s --location --request POST --user \"${auth}\" -H Content-Type:application/sparql-update --data-binary \"@${file}\" ${update}"
				begin=$SECONDS
        curl -s --location --request POST --user "${auth}" -H Content-Type:application/sparql-update --data-binary "@${file}" ${update}
				log -v "update $file time=$(($SECONDS-$begin))"
      else
        log -v "update $file not found"
      fi
    done
  else
    err 1 "No harvest-db specified, via --db= or via HARVEST_DB env. variable."
  fi
}

: <<='cut'
=pod

=head2 COMMAND

harvest I<options> B<load> files

Use http to load our data files into the harvestdb.  The harvestdb is specified
as a global environment.

=head3 load OPTIONS

=over 4

=item B<--graph=I<graph_url>>

Select the graph to insert the data.

=item B<--suffix=I<ttl|jsonld>>

if you include stdin, specify the filename suffix to use, eg. C<--suffix=ttl>
will identify C<stdin> as C<file.ttl>.  This can help the data endpoint to
understand the format.  =back

=back

=cut

function load () {
  local opts;
  if ! opts=$(${G[util_getopt]} -o g:f: --long graph:,format: -n 'harvest load' -- "$@"); then
    echo "Bad Command Options." >&2 ; exit 1 ; fi

  eval set -- "$opts"

  declare -A CMD=(
    [graph]="http://experts.ucdavis.edu/nograph/"
    [suffix]="ttl"
    );

  while true; do
	  case $1 in
	    -g | --graph) CMD[graph]=$2;  shift 2;;
	    -s | --suffix) CMD[suffix]=$2;  shift 2;;
	    -- ) shift; break;;
	    *) shift; break;
    esac
  done

  local file;
	local count;
	local begin;

  if [[ -n ${G[harvest_db]} ]]; then
    local auth=${G[fuseki_auth]}
    local load=${G[fuseki_host]}/${G[harvest_db]}/data

    for file in "$@"; do
      if [[ "$file" = '-' ]]; then
        file=/dev/stdin
        fn=file.${CMD[suffix]}
      else
        fn=$file
      fi
			begin=$SECONDS
      log -v -v "curl -s --location --request POST --user ${auth} -H \"Content-Type:multipart/form-data\" -F \"file=@${file};filename=${fn}\" \"${load}?graph=${CMD[graph]}\""
      count=$(curl -s --location --request POST --user ${auth} -H "Content-Type:multipart/form-data" -F "file=@${file};filename=${fn}" "${load}?graph=${CMD[graph]}" | jq .count)
			log -v "load $fn graph=${CMD[graph]} count=$count time=$(($SECONDS-$begin))"
    done
  fi
  # No can load both
  if [[ -n ${G[tdb]} && false ]]; then
    for file in "$@"; do
      [[ "$file" = '-' ]] && file=/dev/stdin
      log -v -v "tdb2.tdbloader --loc=${G[tdb]} --graph=${G[graph]} $file"
      count=$(tdb2.tdbloader --loc=${G[tdb]} --graph=${G[graph]} $file)
			log -v "tdb $fn graph=${CMD[graph]} => $count"
    done
  fi
}

<<='cut'
=pod

=head2 COMMAND

harvest I<options> B<xslt> []

Runs the old VIVO_Harvester processing. This B<only> runs in /usr/local/vivo/harvester/data

=head3 xslt OPTIONS

=over 4

=item B<--select=I<filename>>

After processing, run a select on the TDB database, and save to the file.

=back

=cut

function xslt() {
  local opts;
  if ! opts=$(${G[util_getopt]} -o nrs: --long no-reprocess,reprocess,select: -n 'xslt' -- "$@"); then
    echo "Bad Command Options." >&2 ; exit 1 ; fi

  eval set -- "$opts"

  local select
  local reprocess=1
  local d=/usr/local/vivo/harvester
  local update="tdbupdate --loc=${d}/data/tdb-output/1 --update=-"
  local query="tdbquery --loc=${d}/data/tdb-output/1 --query=-"

  while true; do
	  case $1 in
	    -r | --reprocess) reprocess=1;  shift 2;;
	    -n | --no-reprocess) reprocess='';  shift 2;;
	    -s | --select) select=$2;  shift 2;;
	    -- ) shift; break;;
	    *) shift; break;
    esac
  done

  function initialize_state_txt() {
    echo '0'
    date --utc --iso-8601=sec | sed -e 's/\+00:00/+0000/'
    echo '0'
  }

  if [[ -n $reprocess ]]; then
    initialize_state_txt > $d/data/state.txt
    rm -rf ${d}/data/tdb-output
	  if ! ${d}/elementsfetch.sh --reprocess; then
      err "elementsfetch failed"
    fi
  fi

  if [[ -n $select ]]; then
	  ${update} <<<'delete WHERE {?s ?p <http://experts.ucdavis.edu/ontology/local#InternalClass>. }'
	  ${update} <<<'PREFIX vivo: <http://vivoweb.org/ontology/core#> delete {?s ?p ?o. } WHERE {VALUES (?d) {(vivo:University)(vivo:AcademicDepartment)} ?s a ?d ;?p ?o .}'
	  ${query} <<<'construct {?s ?p ?o. } WHERE {?s ?p ?o .}' > ${select}
  fi

}

: <<='cut'
=pod

=head1 DEPENDANCIES

Elements uses a number of external bash commands. These must be installed for
the elements script to work.

=over 4

=item L<httpie|https://httpie.org/>

httpie is a command-line tool similar to B<curl>. Since we only really need the
authentication, it may be better to use curl here and the .netrc file instead.
It is nice to have the httpie interface however, for debugging.

=item L<getopt>

${FLAGS_GETOPT_CMD:-getopt}

=back

=head1 AUTHOR

Quinn Hart <qjhart@ucdavis.edu>

=cut


OPTS=();
while true; do
	case $1 in
	  -*) OPTS+=($1); shift ;;
	  -- ) shift; break;;
	  *) break;
	esac
done

main.init "${OPTS[@]}"
main.cmd "$@"

exit 0;
