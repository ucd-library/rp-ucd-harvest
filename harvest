#! /usr/bin/env bash

: <<=cut
=pod

=head1  NAME

harvest - overall harvest command

=head1 SYNOPSIS

harvest [-b|--base=<base>]
  <command> [<args>]

  where <command> is one of xslt


=head1 GLOBAL OPTIONS

=over 4

=item B<-b|--base = directory>

Specify the base directory for your feed results. Default C<base=./>.


=item B<-v|--verbose>

Log some of the commands that your are going to run.

=item B<-h|--help>

Shows the manpage for the program. The help pages are embedded in the script and
require the functions, C<pod2usage> and C<pod2text> to work properly.

=back

=cut

function main.init() {

  #MAIN
    # global Variables
    declare -g -A G=(
      [harvest_home]="$( cd "$( dirname "${BASH_SOURCE[0]}" )/../lib/harvest" >/dev/null 2>&1 && pwd )"
      [iam_api_endpoint]="iam.api:=https://iet-ws.ucdavis.edu/api/iam"
      [iam_search]=""
      [fuseki_host]='http://fuseki:3030'
      [harvest_db]=''
      [fuseki_admin]='http://fuseki:8080'
      [fuseki_user]='admin'
      [fuseki_password]=''
      [fuseki_auth]=''
      [util_getopt]=${HARVEST_UTIL_GETOPT:-${FLAGS_GETOPT_CMD:-getopt}}
    );

    local opts;
    if ! opts=$(${G[util_getopt]} -o f:b:vh --long db:,fuseki-host:,fuseki-auth:,base:,verbose,help -n 'harvest' -- "$@"); then
    echo "Bad Command Options." >&2 ; exit 1 ; fi

    eval set -- "$opts"

    local i
    declare -A CMD;
    while true; do
	    case $1 in
	      -b | --base) CMD[base]=$2;  shift 2;;
	      -f | --fuseki-host) CMD[fuseki_host]=$2;  shift 2;;
	      -d | --db) CMD[harvest_db]=$2;  shift 2;;
	      --fuseki-admin) CMD[fuseki_admin]=$2;  shift 2;;
	      -a | --fuseki-auth) CMD[fuseki_auth]=$2;  shift 2;;
	      -v | --verbose) CMD[verbose]=1;  shift;;
        -h | --help ) exec pod2text $0;;
	      -- ) shift; break;;
	      *) shift; break;
      esac
    done

    # system variables (ELEMENTS_FOO_BAR) over config file
    # the HARVEST command uses root variables for these input, that is, eg
    # FUSEKI_PASSWORD, etc.
    for i in "${!G[@]}"; do
      eval v=\$${i^^}
      [[ -n $v ]] && G[$i]=$v
    done

    # command line over config and over system var
    for i in "${!CMD[@]}"; do
      [[ -n ${CMD[$i]} ]] && G[$i]=${CMD[$i]};
    done

    # Now build an auth if needed
    [[ -n ${G[fuseki_auth]} ]] || G[fuseki_auth]="${G[fuseki_user]}:${G[fuseki_password]}"

}

: <<='cut'

=pod

=head1 COMMANDS

Next there are a set of commands that communicate with the CDL service. Note,
that ezid uses L<httpie|https://httpie.org/> for its http communcation. This
allows users to combine ezid with other httpie requests if required. Login
infomation is stored using the standard C<httpie> methodology, see L</"GLOBAL OPTIONS"> for httpid options.

C<elements [options] harvest --full> Harvests a number of feeds from the
Elements database and stores each record in a separate file.  This is to match how
the Sympletic Harvester works

=cut

function main.cmd () {
    cmd=$1
    shift;
    case $cmd in
	    xslt) # VIVO Harvester
	      $cmd "$@";
	      ;;
      db | load | fuseki | update | query ) # Fuseki commands
        $cmd "$@";
        ;;
      config ) # informational requests
        _${cmd} "$@";
        ;;
	    *)
	      exec pod2usage $0
	  ;;
    esac
}

function log() {
  [[ -n ${G[verbose]} ]] && (>&2 echo LOG: $@)
}

function err() {
  local n=1;
  if [[ $1 = '--quiet' ]] ; then
    n=$2;
  else
    n=$1
    shift
    (>&2 echo err: $@)
  fi
  exit $n;
}

: <<='cut'
=pod

=head2 COMMAND

harvest I<options> B<db> list|rm|new [service]

Performs processes on the fuseki instance.  These are command-line overlays to
the harvetstdb admin API in the ucd-rp-fuseki host.


=head3 COMMANDS

=over 4

=item B<list>

List all fuseki harvest services.  The list comes from the existing harvest
databases, not the services running in the server.

=item B<new> [service]

Add a new harvest service.  This will add a standard database, with a private
section, as well as access to the public data.

=item B<rm> [service]

Delete a harvest service.  This will remove the configuration, and the
underlying database file.  This takes advantage of the L<fuseki HTTP adminsration protocol|https://jena.apache.org/documentation/fuseki2/fuseki-server-protocol.html>

=back

=cut

function db() {
  local cmd

  local auth=${G[fuseki_auth]}
  local dbs=()

  cmd=$1
  shift;

  case $cmd in
    datasets)
      http --print=b --auth=${auth} GET ${G[fuseki_host]}/\$/datasets | jq '.datasets[]["ds.name"]'
      ;;
    list)
      http --print=b --auth=${auth} GET ${G[fuseki_admin]}/harvestdb
      ;;
    rm)
      while [[ -n "$1" ]]; do
        dbs+=$(http --auth=${auth} DELETE ${G[fuseki_admin]}/harvestdb?name=$1)
        shift
      done
      ;;
    new)
      if [[ -z "$1" ]]; then
        dbs+=$(http --auth=${auth} POST ${G[fuseki_admin]}/harvestdb)
      else
        while [[ -n "$1" ]]; do
          dbs+=$(http --auth=${auth} POST ${G[fuseki_admin]} /harvestdb?name=$1)
          shift
        done
      fi
      ;;
    *) err "Invalid command $cmd"
       ;;
  esac
  echo "${dbs[@]}"
}


: <<='cut'
=pod

=head2 COMMAND

harvest I<global_options> B<run> I<options>

Run a complete harvest operation for a particular ucdid search.  This command
will first initialize a HARVEST_DB if one does not exist.  Then the program will
query the C<ucdid> with the the passed C<--search> query, and collect a set of
users.  It will then contact the CDL elements database (via the cdl command) and
download all user and publication information for these users.  Then the command
will run a series of sparql updates (via C<harvest update>) to update the
experts database.  The command will then clean up, by removing the HARVEST_DB,
and the cache of files downloaded from the services.

=over4

=item B<--init>

Specify whether to initialize a new database.  By default, this will check to
see if there is a global database already set.  If not, then this will
initialize a new database.  If a global database exists, then use that.  This
can be set either with the B<HARVEST_DB> environment variable, or with the
C<--db=I<harvest_db>> global option.  If C<--init> is set, then create a new
harvest_db regardless of the existance of a current HARVEST_DB.

=item B<--search=<ucdid --search parameter>

Specify the initial ucdid query to get the list of users to harvest, and update.
This is passed as the C<--search> to the C<ucdid> command.  All users from this
command will be included in the harvest operations.

=item B<--last-modified=<only included items changed since this date-time>

The C<--last-modified> takes a ISO datetime string, and only includes items that
have been modified since that date.  If unspecified, all items from the query
will be updated.

=item B<--remove|--no-remove>

Specify whether to remove the HARVEST_DB service and the resultant cache files
after processing.

=back

=cut

function run () {
  local opts;
  if ! opts=$(${G[util_getopt]} -o g:f: --long graph:,format: -n 'harvest load' -- "$@"); then
    echo "Bad Command Options." >&2 ; exit 1 ; fi

  eval set -- "$opts"

  declare -A CMD=(
    [init]=''
    [search]=''
    [remove]=1
    [modified]=''
    );

  while true; do
	  case $1 in
	    -i | --init) CMD[init]=1;  shift;;
	    -s | --search) CMD[search]=$2;  shift 2;;
      -r | --remove) CMD[remove]=1; shift;;
      -n | --no-remove) CMD[remove]=''; shift;;
      -m | --last-modified) CMD[modified]=$2; shift 2;;
	    -- ) shift; break;;
	    *) shift; break;
    esac
  done

  if [[ -z $CMD[search] ]]; then
    err "harvest run, must specify a --search parameter.  See ucdid for details."
  fi

  if [[ -n ${CMD[init]} || -z ${G[db]} ]]; then
    G[db]=db new;
    log "harvest new db: ${G[db]}"
  fi

  # Now fetch all the data w/ CDL
  local cache=${G[base]}/${G[db]}
  mkdir $cache;


  # call UCDID w/ query
  local profiles_ttl=$cache/profiles.ttl
  ucdid fetch --format=ttl --search="${CMD[search]}" profiles > $profiles_ttl
  load --graph=http://iam.ucdavis.edu/ $profiles_ttl

  # Now run an update to convert IAM to vivo users
  update iam_to_vivo

  # Now get all the casIDs, This doesn't use experts, but the harvestdb
  ids=$(aeq --endpoint=${HARVEST_ENDPOINT}/sparql query --format=json <<<"select ?id where { graph harvest_iam: {[] iam:userID ?id. }} order by ?id" | jq -r .results.bindings[].id.value | tr [:space:] ' ')


  # cdl --data=$HARVEST_ENDPOINT -v login --auth=${CDL_AUTH}
  cdl --cache=$cache --data=${G[endpoint]}/${G[db]} users --pubs $ids

  # Then, run the update scripts for new data
  update update_harvest_records;
  update journal
  update publications
  update concepts
  update keywords

}



: <<='cut'
=pod

=head2 COMMAND

harvest I<global_options> B<query> I<options> file1.ttl file2.ttl ...

Run queries on the fuseki harvestdb endpoint.  I

=cut

#function query () {
#}


: <<='cut'
=pod

=head2 COMMAND

harvest I<global_options> B<update> I<options> file1.ttl file2.ttl ...

Run update queries on the fuseki harvestdb endpoint.  I

=cut

function update () {
  local file;
  if [[ -n ${G[harvest_db]} ]]; then
    local auth=${G[fuseki_auth]}
    local update=${G[fuseki_host]}/${G[harvest_db]}/update

    log ${update}
    # Short hand for files in lib/harvest
    for file in "$@"; do
      [[ "$file" = '-' ]] && file=/dev/stdin
      if [[ ! -f $file ]]; then
        file=${G[harvest_home]}/rq/$file;
        [[ -f $file ]] || file=${file}.ru
      fi
      if [[ -f $file ]]; then
        log "curl -i --location --request POST --user \"${auth}\" -H Content-Type:application/sparql-update --data-binary \"@${file}\" ${update}"
        curl -i --location --request POST --user "${auth}" -H Content-Type:application/sparql-update --data-binary "@${file}" ${update}
      else
        log $file not found
      fi
    done
  else
    log "No harvest-db specified, via --db= or via HARVEST_DB env. variable."
  fi
}

: <<='cut'
=pod

=head2 COMMAND

harvest I<options> B<load> files

Use http to load our data files into the harvestdb.  The harvestdb is specified
as a global environment.

=head3 load OPTIONS

=over 4

=item B<--graph=I<graph_url>>

Select the graph to insert the data.

=item B<--suffix=I<ttl|jsonld>>

if you include stdin, specify the filename suffix to use, eg. C<--suffix=ttl>
will identify C<stdin> as C<file.ttl>.  This can help the data endpoint to
understand the format.  =back

=back

=cut

function load () {
  local opts;
  if ! opts=$(${G[util_getopt]} -o g:f: --long graph:,format: -n 'harvest load' -- "$@"); then
    echo "Bad Command Options." >&2 ; exit 1 ; fi

  eval set -- "$opts"

  declare -A CMD=(
    [graph]="http://experts.ucdavis.edu/nograph/"
    [suffix]="ttl"
    );

  while true; do
	  case $1 in
	    -g | --graph) CMD[graph]=$2;  shift 2;;
	    -s | --suffix) CMD[suffix]=$2;  shift 2;;
	    -- ) shift; break;;
	    *) shift; break;
    esac
  done

  local file;
  if [[ -n ${G[harvest_db]} ]]; then
    local auth=${G[fuseki_auth]}
    local load=${G[fuseki_host]}/${G[harvest_db]}/data

    for file in "$@"; do
      if [[ "$file" = '-' ]]; then
        file=/dev/stdin
        fn=file.${CMD[suffix]}
      else
        fn=$file
      fi
      log "curl --location --request POST --user ${auth} -H \"Content-Type:multipart/form-data\" -F \"file=@${file};filename=${fn}\" \"${load}?graph=${CMD[graph]}\""
      curl --location --request POST --user ${auth} -H "Content-Type:multipart/form-data" -F "file=@${file};filename=${fn}" "${load}?graph=${CMD[graph]}"
    done
  fi
  # No can load both
  if [[ -n ${G[tdb]} ]]; then
    for file in "$@"; do
      [[ "$file" = '-' ]] && file=/dev/stdin
      log "tdb2.tdbloader --loc=${G[tdb]} --graph=${G[graph]} $file"
      tdb2.tdbloader --loc=${G[tdb]} --graph=${G[graph]} $file
    done
  fi
}

<<='cut'
=pod

=head2 COMMAND

harvest I<options> B<xslt> []

Runs the old VIVO_Harvester processing. This B<only> runs in /usr/local/vivo/harvester/data

=head3 xslt OPTIONS

=over 4

=item B<--select=I<filename>>

After processing, run a select on the TDB database, and save to the file.

=back

=cut

function xslt() {
  local opts;
  if ! opts=$(${G[util_getopt]} -o nrs: --long no-reprocess,reprocess,select: -n 'xslt' -- "$@"); then
    echo "Bad Command Options." >&2 ; exit 1 ; fi

  eval set -- "$opts"

  local select
  local reprocess=1
  local d=/usr/local/vivo/harvester
  local update="tdbupdate --loc=${d}/data/tdb-output/1 --update=-"
  local query="tdbquery --loc=${d}/data/tdb-output/1 --query=-"

  while true; do
	  case $1 in
	    -r | --reprocess) reprocess=1;  shift 2;;
	    -n | --no-reprocess) reprocess='';  shift 2;;
	    -s | --select) select=$2;  shift 2;;
	    -- ) shift; break;;
	    *) shift; break;
    esac
  done

  function initialize_state_txt() {
    echo '0'
    date --utc --iso-8601=sec | sed -e 's/\+00:00/+0000/'
    echo '0'
  }

  if [[ -n $reprocess ]]; then
    initialize_state_txt > $d/data/state.txt
    rm -rf ${d}/data/tdb-output
	  if ! ${d}/elementsfetch.sh --reprocess; then
      err "elementsfetch failed"
    fi
  fi

  if [[ -n $select ]]; then
	  ${update} <<<'delete WHERE {?s ?p <http://experts.ucdavis.edu/ontology/local#InternalClass>. }'
	  ${update} <<<'PREFIX vivo: <http://vivoweb.org/ontology/core#> delete {?s ?p ?o. } WHERE {VALUES (?d) {(vivo:University)(vivo:AcademicDepartment)} ?s a ?d ;?p ?o .}'
	  ${query} <<<'construct {?s ?p ?o. } WHERE {?s ?p ?o .}' > ${select}
  fi

}

: <<='cut'
=pod

=head1 DEPENDANCIES

Elements uses a number of external bash commands. These must be installed for
the elements script to work.

=over 4

=item L<httpie|https://httpie.org/>

httpie is a command-line tool similar to B<curl>. Since we only really need the
authentication, it may be better to use curl here and the .netrc file instead.
It is nice to have the httpie interface however, for debugging.

=item L<getopt>

${FLAGS_GETOPT_CMD:-getopt}

=back

=head1 AUTHOR

Quinn Hart <qjhart@ucdavis.edu>

=cut


OPTS=();
while true; do
	case $1 in
	  -*) OPTS+=($1); shift ;;
	  -- ) shift; break;;
	  *) break;
	esac
done

main.init "${OPTS[@]}"
main.cmd "$@"

exit 0;
